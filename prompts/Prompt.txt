# Onboarding - prompt-design spec

You are a Design Agent. You will be focused on designing prompts for our project. First, I'd like you to review the file, .cursor/specs/prompt-design/design.md.

For your first task, I'd like you to design a system prompt to have the LLM guide the user to answering questions that allows the LLM to provide the information needed to determine how ready the user is to retire. The tool you're looking to have the model call is defined in server/agents/tools/retirement.py, and specifically, is the `retirement_readiness` function, where the parameters that the model will be providing to the tool is described in the docstring for the function. The system prompt is to be placed in server/tools/.chat/system-prompts/readiness.txt. The LLM should only guide the conversation with the user in this direction only if the user has expressed interest in understanding when they should retire and/or how ready they are to retire. Once that occurs, then the LLM should ask the necessary questions to pass ALL parameters needed by the tool. Only when all the necessary data is available should the LLM invoke the tool.

First, ask me at least two questions you might have to ensure you better understand the overall purpose of this prototype, and/or the goals of the overarching product, and/or requirements that affect this prompt design.

## Answers to your questions

1. The overarching product purpose is that the LLM is acting as a "retirement consultant". The scope of their coaching is any aspect of retirement, which may include retirement readiness, financial planning, tax planning, debt, estate planning, and any number of different retirement-related issues. This specific focus on "retirement readiness" is really an attempt to test how the LLM can guide the conversation towards invoke the tool, as part of early prototyping. We will be introducing other tools over time, and the prompt will then be expanded to support the invocation of other tools as well.

2. For now, we're only introducing enough in the system prompt to let the model know that they're a "retirement consultant", and if the user is interested in the topic of retirement readiness, the necessary data needed to run the tool should be obtained from the user so that the tool can be invoked, and its response provided to the user.

3. The target audience are US-based mid-to-late career workers near retirement, in any industry.

Let me know if you have any other questions. If not, please proceed to construct the system prompt.

# Data Completeness

I'd like you to now design a new prompt and a corresponding tool to support this prompt. The goal is to have the model interact with the user to obtain and store essential information needed to furnish a plan for retirement.

First, a quick definition of what "essential information" means. Essential information is categories of information that can affect how retirement planning and execution works. In tmp/Essential Topics.md, I have pulled together a 8 topics, and within them, critical sub-topics that cover some of the most important topics for the user to consider. Not all such topics are important for every user.

What I'd like you to do is to 
1/ design a system prompt, server/tools/.chat/system-prompts/completeness.txt, that is similar to the readiness.txt system prompt that you had previously designed (same persona for the model, same target audience), except instaed of steering the user towards the "retirement readiness" tool, the goal is to cover a breadth of topics described in the "Essential Topics" described in the "Essential Topics.md" file. 
    1a/ Topics. The model need not cover every single topic in depth, but should instead explore with the user, what topics are important to them, and for topics that are important, dive deeper to ensure that the user understands the issues relating to those topics of importance to them. 
    1b/ Information. Whenever the model obtains new information about the user's circumstances, goals, or needs for any of the topics, call a new "information" tool (defined later), and provide the new information to be persisted, identifying which topic(s) they apply to.
    1c/ Completeness. Following every 3-5 new information tool invocation, the model should also invoke a "information-query" tool, that provides all the persisted information, to then allow the model to make an assessment of how complete information gathering has been for the user in each of the topics, on a 0-100 scale.
2/ design a tool, called "information", that would be invoked by the model with a `role: "tool"` response from the model, that includes, as parameters, which topic, and which subtopic (if applicable), that new information has been obtained for the user, and to persist this data. This tool is to be placed in the server/agents/tools folder, as "information.py", and invoked through interactions through the server/tools/chat CLI.
3/ design a tool, called "information-query", that would be invoked by the model with a `role: "tool"` response from the model. This tool will fetch all persisted information stored by the "information" tool. This tool is to be placed in the server/agents/tools folder, as "information-query.py", and invoked through interactions through the server/tools/chat CLI.
4/ design a tool, called "completeness", that would that would be invoked by the model with a `role: "tool"` response from the model, that includes, as parameters, for each topic, the model's assessment of completeness, and to persist this completeness data. This tool is to be placed in the server/agents/tools folder, as "completeness.py", and invoked through interactions through the server/tools/chat CLI.

The system prompt is something you can directly design and place in the server/tools/.chat/system-prompts folder. The tools themselves will be implemented by the coding agent. What you will need to do is to specify the tools in a new file, .cursor/specs/prompt-design/design-completeness-tools.md. You should use the structure and format of the .cursor/specs/prompt-design/design.md to help you articulate the design of these tools.

Before you get started, I'd like you to ask me at least 5 questions on this Data Completeness design.

## Answers to clarifying questions

1. This is an early prototype. We should pick a simple persistence approach that allows for easy writing, appending, and fetching of the data. I'm happy to explore options, but I was thinking that a jsonl file would suffice for this. I think storing this information in server/tools/.chat/sessions/<session-id> would work.

2. The topic should be strict enums. The subtopics could be free-form, to provide flexibility to the model.

3. A more structured payload will be important, but still leave the model to have some freedom of the content within the structure. In your example, the typs of facts in `fact_type` could be determined by the model, but capturing timestamp, and confidence is something we want to prescribe.

4. The basis for updating updating completeness scoring is when there is material changes to completeness. As a conceptual guideline, I'd say that we need to update completeness with every 10 score improvement across any of the topics, or every 3 or so information updates. We should parameterize this threshold, and once we've had some examples of how quickly score improvement changes, we can then tune these threshold parameters.

5. I think we'll want some level of flexibility to allow the model to decide when the check in. But as I mentioned in 4, the goal is to tune this as we explore this further.

6. Let's not include the completeness score in the chat response to the user. The overarching design intent here is that the completeness scoe is presented through a different non-chat interface (a completeness chart, some other clickable experience showing the user what information has already been gathered). For this prototype, we will be able to see tool calls in the CLI, so we'll be able to observe the model's tool actions.

I suspect there's still some clarification needed, so please ask me at least another question to help you understand this better.

7. Everything to stay within the session. No cross-session support needed right now for the prototype.

If you don't any other questions, please proceed to design the system prompt, and then create the .cursor/specs/prompt-design/design-completeness-tools.md document to provide the Coding Agent guidance on the completeness tools they need to implement.

## Review of designs

completeness.txt
* In the second paragraph, starting with "Only steer the conversation...", I would strengthen the "data completeness" flow by having the model ALWAYS evaluate "data completeness". However, we do not want to force the user to cover any specific topic, nor the depth of exploration in any specific topic. What we'd want to do with this particular flow is to capture information that users want to provide, and the completeness of information along any (or all) of the eight topic areas. You should look through other text in the system to adjust the prompt according to this guidance (eg., in the third paragraph, starting with "When the user is interested...", you should instead state that this is the goal of this flow, ie., to explore what the user's retirement plan should encompass).
* In the system prompt, there's a reference to "Essential Topics". This is a reference to a file that the LLM does not have access to. That said, we'll use a reasonably well capable LLM that may already understand these concepts if we provide enough information about what the topic covers. The question is whether we should include some (or all) of the information in the tmp/Essential Topics.md file in the system prompt, or whether we should provide a bit more details in the system prompt, but ultimately, rely on the LLM's trained knowledge of each of these topics. In either event, we should not use the term "Essential Topics" as this reference does not make sense in the system prompt without the corresponding file. 

I'd like to get your feedback, especially for the second bullet point. One thing I'm not clear on is whether we can/should reliably assume that the model will cover critical sub-topics. For instance, "Social Security Claiming Strategy" can be quite significant when it comes to Cash Flow. Can we rely on the model to explore this topic with the user, or should we place a greater emphasis on it, while still allowing the model some latitude to explore other sub-topics?

## Housekeeping

Please go ahead and commit and push ALL changes and untracked files of the following - the .cursor/, and the server/ folders.

## tasks-completeness-tools.md

I'd like you to create a new file, .cursor/specs/prompt-design/tasks-completeness-tools.md. It should mirror the structure and format of the tasks.md file in the same folder. The goal here is for you to document how you would like the Coding Agent to implement the tools you have just defined in .cursor/specs/prompt-design/design-completeness-tools.md. The Coding Agent will be asked to review the design document first, and then execute the implementation in the way you describe in the corresponding tasks document. This will give you some control over how the task is implemented, manage dependencies, and have checkpoints where you can review their work and request changes without waiting for all tasks to be completed before being able to review their work. I generally have a review with Design (which is your role for prompt work) at the completion of every major task number (eg., Task 1, Task 2, etc.).

Before you get started, please ask me at least one question to clarify the process or to understand how to document tasks.

### Response to clarifying questions

The tasks will be read by the AI Coding Agent. If you think it's necessary to be more prescriptive and descriptive in defining details, then feel free to break the task down into smaller chunks of work. However, the AI Coding Agent is a very competent AI model (GPT-5.1-Codex), so it's unlikely that they'll need too much detail, unless you believe that there are nuances that need to be called out. The more important thing to do is to make sure that the major (numbered) tasks are clearly separated to give you (and I) the opportunity to do a review and suggest course corrections, in case the design document provided insufficient specificity.

## Task 1 review

Coding Agent has completed Task 1. They have not committed the code changes yet, so go ahead and have a look at the changed code to determine whether they have implemented to your spec for Task 1. 

Let's get the Coding Agent to switch to typed dicts. Please give me the necessary prompt to send to the Coding Agent in a chat to have them rework Task 1 to support typed dicts.

Please check the Coding Agent's latest revision (again, same files, not committed) to see if they've implemented typed dicts, consistent with the design spec.

## Task 2 question

Question from teh Coding Agent. "When information_query reads information.jsonl, should it return the raw records in chronological order as stored (append order), or do you want it to apply any sorting/filtering (e.g., most recent first, by topic) before handing them back to the LLM?"

Does the order that we send the raw records to the LLM matter for the purpose of it doing its processing to determine completeness? Would the LLM be more effective if we've organized the data by topic, or by recency?

## Task 2 review

Coding Agent has completed Task 2. They have not committed the code changes yet, so go ahead and have a look at the changed code to determine whether they have implemented to your spec for Task 2. 

## Task 3 review

Coding Agent has completed Task 3. They have not committed the code changes yet, so go ahead and have a look at the changed code to determine whether they have implemented to your spec for Task 3. 

## Feedback on prompt

Question - don't action yet. Have a look at server/tools/.chat/sessions/dc189f3b-7c9b-4f34-9577-eb7422de7f17/history.json. After quite a number of turns focused on cash flow, I don't see a completeness tool invocation. Would you expect that after this many exchanges, completeness tool would have been called already, given the system prompt you provided? Discussion only for now.

Now, have a look at the same file again. This time round, I've provided a bunch more information, and we're still not getting a completeness tool call. At what point do we conclude that the model is not going to call it, and the system prompt needs to be reworked. Discussion only - don't take any action yet.

I'd like you to make changes so that the completeness call is made at least once on the first tool information call, and then on at least every 3 tool information calls thereafter. But don't commit the change. I'd like to see what happens in a new session.

I see errors in the logs for the completeness tool calls. Can you have a look at server/tools/.chat/sessions/a28dbb73-7567-4fcf-a335-59fc699c9650/history.json to see whether this is due to how the model is calling the tool, or perhaps the tool itself has an issue.

Okay, then please proceed to update the `completeness.txt` system prompt to ensure that the model calls the tool properly.

A new run. This time, I'm see one failed completeness tool call, followed by two successful ones. Can you check server/tools/.chat/sessions/31805662-3844-4a3b-ade6-ecb084061427/history.json to see if the failed one requires any change in the system prompt?

Check out the history logs, server/tools/.chat/sessions/31805662-3844-4a3b-ade6-ecb084061427/history.json, and the completeness data, server/tools/.chat/sessions/31805662-3844-4a3b-ade6-ecb084061427/completeness.jsonl. While much of our focus has been on cash flow, I have also started providing residency information (living in California, exploring options for living outside the country and NV). I also provided my property information (primary, second). These should theoretically cause some progress in completeness in "Housing & Geography" topic. Is it possible that the model became too focused on cash flow and is neglecting the information they're starting to collect in the other 7 topics? I can try explicitly asking questions in the other 7 topics next to see if that helps, or perhaps you can give me some specific user prompts to help test this?

Have a look at the latest entry in server/tools/.chat/sessions/31805662-3844-4a3b-ade6-ecb084061427/completeness.jsonl. Here, the model adds a single entry into the completeness tool for just the "estate_planning", whereas, in previous entries, they provide a cumulative data of all topics they have information for. Could the reason for this be that the model will eventually lose context about prior topics? Should we ensure consistency in how completeness data is accumulated by not having the model be responsible for tracking other topics, and do the accumulation deterministically? Discussion only. Do not modify any files yet.

Okay, in that case, I think we have a good enough working prototype. Please go ahead and commit and push the server/ folder (on the current branch `completeness-tools`), and the .cursor/ folder (on the main branch).

## Task 4.1

Okay, let's proceed on Task 4.1. Throughout the development of tasks 1-3, I've had you review what's been implemented. Given Task 4.1 is about reviewing the work done on Tasks 1-3, do you have views on what else we need to implement to complete the efforts of Tasks 1-3?

I have some request for new capabilities that overlap somewhat with some of the refinements you list. Have a read of them, and we can A/ add these as new tasks (eg., Tasks 5), B/ add these as additions to the existing design, and then subsequently, add these as new tasks, or C/ embed this within Task 4.1 (likely modifying Task 4.1).

1. Create a CLI tool, server/tools/completeness, that displays the current completeness data. This tool should 
    1a. provide a display that fetches the latest data from the completeness.jsonl file of the current session (if not optional --session is not provided), or a specific session (if the optional --session SESSION is provided where SESSION is the session id).
    1b. the completeness.jsonl is examined every two seconds for changes (potentially by looking at the update timestamp) and if there are changes, the latest data is fetched
    1c. for each of update, the terminal screeen is cleared, and then the new data is presented
    1d. the data is presented in the following way:
        * for each of the 8 topics, listed in a fixed and unchanging order the latest score is presented using a text arrow after the name of the topic, where which character represents 5 points on the scores
        * as examples, the following shows "income_cash_flow" with score 85 (16 `=` plus 1 `>` totalling 17 characters or 85 points), and so forth, and with the `|` representing the start of the arrow, and with all `|` symbols aligned for every line representing a topic, and after the `>` symbol, a space followed by the number representing the score, and scores of zero is represented by `| 0`:
            1. income_cash_flow     |================> 85
            2. healthcare_medicare  |========> 45
            3. housing_geography    |===========> 60
            4. tax_efficiency_rmds  | 0
            5. longevity_inflation  | 0
            6. long_term_care       | 0
            7. lifestyle_design     | 0
            8. estate_planning      |=========> 50
    1e. and after the last topic line, provide a prompt to the user to enter a number to explore more deeply on a specific topic,
        * where the prompt to the user is provided in the following form:
            Help me explore a specific topic (enter number 1-8)> 
        * where if the user does enter a number (1-8 only), causes an active chat tool (server/tools/chat) to send a message on behalf of the user the request that the LLM help them explore more deeply on a specific topic (and you, the Prompt Design Agent should recommend an appropriate user prompt to achieve this)
    1f. where the chat-completeness tool is terminated with a ctrl-D or ctrl-C
2. Create a CLI tool, server/tools/profile, that displays the accumulated information about the user. This tool should
    2a. provide a display that fetches the latest data from the information.jsonl file of the current session (if not optional --session is not provided), or a specific session (if the optional --session SESSION is provided where SESSION is the session id).
    2b. the information.jsonl is examined every two seconds for changes (potentially by looking at the update timestamp) and if there are changes, the latest data is fetched
    2c. for each of update, the terminal screeen is cleared, and then the new data is presented
    2d. the data is presented in the following way:
        * for each topic, display information gathered about the user grouped by topic, then subtopic, then each entry in chronological order 
        * as examples, from the first few entries of server/tools/.chat/sessions/31805662-3844-4a3b-ade6-ecb084061427/information.jsonl, we get the following:
            income_cash_flow 
                retirement_timing
                    Goal: User is considering retiring next year
                    Goal: User and spouse plan to stop working around May next year, with no work income afterward (conservative assumption).
                ages
                    Fact: User is 54; spouse is 6 years younger (age 48).
                spending
                    Goal: target retirement spending is about $200,000 per year (today's dollars).
                social_security
                    Goal: User and spouse plan to claim Social Security at age 70 and expect to receive near-maximum benefits as high earners (based on today's benefit levels).
                assets
                    Goal: User has about $4M in liquid/investable assets and about $4M net equity in property.
                asset_breakdown
                    Fact: Liquid assets: ~$1.5M taxable, ~$2.3M in 401k/IRA, ~$200k in Roth. Property: primary home ~$2.5M with $700k debt, second home ~$1.3M with $900k debt, two rentals ~$900k each with no debt. Rental income ignored for now.
            housing_geography
                future_moves
                    Goal: User plans to sell one rental property in about 2 years, and sell the current primary residence and move into the second home in about 7 years.
            healthcare_medicare
                priority
                    Preference: User wants to focus on healthcare insurance options and cost implications given early retirement at 54/48 with significant assets.
I'm not sure whether 1e is possible (ie., having two different CLI scripts send event signals to each other), but let's discuss what is possible.

Have described what I'm looking for, I'd like you to ask me at least three questions to clarify the requirements for these new tools.

### Answers to Clarifying Questions.

1. I'm envisaging a demonstration in the IDE, where there are three windows, each running separate terminals, where one terminal runs the existing chat tool, another the new completeness tool, and the third, the new profile tool. In the case of 1e, the chat tool is running, providing an interactive conversation with the LLM. Ideally, the completeness tool somehow causes the chat tool to send the message as the user on the already running chat tool. That's why I was asking whether it's possible for the completeness tool to send an event or signal to an already running chat tool, with all tools being CLI tools, invoked in separate terminals.

2. I'm not sure I understand the RETIRE_CURRENT_SESSION_ID vs. index.json vs. is_current flag. There seems to be too many ways to specify what the current session id is. As a user, I'm only familiar with the session that the `chat --list-sessions` marks as the current session. That's the one we should be focused on. Please explain all the other methods, and we should discuss whether we need three of them.

3. The tools should not automatically exit. A ctrl-C/D is needed to terminate the tools. If completeness.jsonl/information.jsonl files do not exist, or do exist but contain no data, then we should show "awaiting data..." message and keep polling.

4. Let's start with the simple ANSI clear screen and monospaced alignment for now. As mentioned in 1 above, each tool will be running in a separate terminal. 

Please respond to my questions above, and if you have follow-up or new questions, please ask me. Also, do you have an opinion on whether these are small enough to simply be added as additional tasks?

### Continued Discussion

1. Okay, let's keep things simple and have the completeness tool print a recommended prompt for the user to copy/paste into the chat terminal.

2. Okay, go with your recommendation.

3. I assume you're not proposing to use TUI. When we tried getting a TUI (prompt_toolkit) to work, the Coding Agent struggled to make it behave, in part I suspect, because they're not able to observe what was being generated, and after several iterations of explaining what I was seeing, they still weren't able to resolve the issue. I assume that you're proposing to do this somehow with a stdout/stdin approach? 

4. Okay.

5. Okay, go ahead and update the design doc, and add Task 5.

Unless you have further questions, please proceed to document these requirements as you suggest in 5.

### Coding Agent Question

Here's a question from the Coding Agent:

'Topic prompts: Task 5.1 says the completeness monitor should print a “recommended user prompt” for whichever topic number the user selects, but the design spec notes the phrasing will come from the Design Agent in a “future iteration.” Do you already have the exact copy for each of the 8 topics, or should I draft provisional text (to be reviewed later)?'

Can you create a JSON file, server/tools/.chat/user-prompts/explore-topic.json, which is simply a JSON file containing pairs of topic names (eg., `income_cash_flow`, `healthcare_medicare`, etc.), and corresponding "recommended user prompt", that the Coding Agent can include as text to copy verbatime?

Then, add to Task 5.1, under the bullet referenced by Coding Agent, an explanation of where to find these "recommended user prompt" and a brief explanation of how to look up the user prompt from the topic names.

# Gemini Prompt

Using the attached screenshot, I'd like you to create a sample web UI experience for a retirement chat app. The app has three panels - the left as shown, is a "chat" interface, with the LLM's completion response to a prior user prompt shown in blue, and the user's prompt (currently empty) shown in green at the bottom. Use the text being shown to populate this chat panel. The user prompt should be provided in a small text area at the bottom of this chat panel, and above it, is the history of the chat interaction between the user and the LLM, with both user and LLM interactions being shown.

The top-right panel is a "completeness" interface, which in the screenshot, shows how complete an understanding the LLM has of the user's retirement situation across 8 different topics. This should be rendered as 8 different circular gauge charts, drawn to show how close to 100% completeness. Any amounts under 40% should be shown in red, 40-80% (inclusive) shown in yellow, and 80-100% shown in green. The center of each of these gauges should be the topic itself, and actual percentage value shown below the topic in a slightly bigger font. The eight circles should be laid out in a circular, around a center text, called "Completness".

The bottom-right panel is a organized log of everything the LLM has uncovered about the user's situation in retirement. Show this in a table organized by topic (eg., `housing_geography`, then topic, eg., `current_home`, then the individual facts or goals).

There should be an overarching web page name at the top. Call it "RetireGPT".

Before you render this web UI experience, ask me at least 3 questions to help you better understand how to render such a a web page.

## Housekeeping

Okay, I think we're good with Task 5. Go ahead and commit all changes in the .cursor folder, and push.